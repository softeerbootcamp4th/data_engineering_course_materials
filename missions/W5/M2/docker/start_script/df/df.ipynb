{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum, avg, count, to_date\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# Spark Master의 IP와 포트 설정\n",
    "spark_master_url = \"spark://spark-master:7077\"  # Spark Master의 IP와 포트\n",
    "\n",
    "# 스파크 세션 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark with Hadoop and Spark Master\") \\\n",
    "    .master(spark_master_url) \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://spark-master:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 데이터 읽기 (예: HDFS에서 Parquet 파일)\n",
    "# df = spark.read.parquet(\"hdfs://spark-master:9000//user/hadoop/input/TLC_Tripdata_Jan_2024.parquet\")\n",
    "# df = spark.read.parquet(\"/Users/admin/Desktop/HMG_W2/missions/W5/M1/docker/untracked/TLC Tripdata Jan 2024.parquet\")\n",
    "\n",
    "# 데이터 로딩\n",
    "df = spark.read.parquet(\"hdfs://spark-master:9000/user/hadoop/input/TLC_Tripdata_Jan_2024.parquet\")\n",
    "\n",
    "# 데이터 클리닝\n",
    "df_clean = df.dropna(subset=[\"base_passenger_fare\", \"trip_miles\"]) \\\n",
    "    .filter((col(\"base_passenger_fare\") > 0) & (col(\"trip_miles\") > 0))\n",
    "\n",
    "# 변환 로직\n",
    "df_transformed = df_clean.select(\n",
    "    col(\"pickup_datetime\").alias(\"date\"),\n",
    "    col(\"base_passenger_fare\").cast(\"double\").alias(\"fare_amount\"),\n",
    "    col(\"trip_miles\").cast(\"double\").alias(\"trip_distance\")\n",
    ")\n",
    "\n",
    "# 날짜 형식으로 변환\n",
    "df_transformed = df_transformed.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# 집계 로직\n",
    "df_aggregated = df_transformed.groupBy(\"date\").agg(\n",
    "    count(\"*\").alias(\"total_trips\"),\n",
    "    _sum(\"fare_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"trip_distance\").alias(\"avg_trip_distance\")\n",
    ")\n",
    "\n",
    "# 전체 집계 결과\n",
    "total_trips = df_transformed.count()\n",
    "total_revenue = df_transformed.agg(_sum(\"fare_amount\")).collect()[0][0]\n",
    "avg_trip_distance = df_transformed.agg(avg(\"trip_distance\")).collect()[0][0]\n",
    "\n",
    "# 일별 집계 결과 표시\n",
    "df_aggregated.show()\n",
    "\n",
    "# 전체 집계 결과 표시\n",
    "print(f\"Total Trips: {total_trips}\")\n",
    "print(f\"Total Revenue: {total_revenue}\")\n",
    "print(f\"Average Trip Distance: {avg_trip_distance}\")\n",
    "\n",
    "# 결과 저장\n",
    "output_path = \"hdfs://spark-master:9000/output\"\n",
    "df_aggregated.write.mode(\"overwrite\").parquet(f\"{output_path}/daily_metrics\")\n",
    "df_transformed.write.mode(\"overwrite\").parquet(f\"{output_path}/cleaned_data\")\n",
    "\n",
    "# CSV 형식으로 저장\n",
    "df_aggregated.write.mode(\"overwrite\").csv(f\"{output_path}/daily_metrics_csv\")\n",
    "df_transformed.write.mode(\"overwrite\").csv(f\"{output_path}/cleaned_data_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 종료\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
