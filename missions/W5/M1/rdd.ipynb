{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Spark Master의 IP와 포트 설정\n",
    "spark_master_url = \"spark://spark-master:7077\"  # Spark Master의 IP와 포트\n",
    "\n",
    "# 스파크 세션 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark with Hadoop and Spark Master\") \\\n",
    "    .master(spark_master_url) \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://spark-master:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 데이터 읽기 (예: HDFS에서 Parquet 파일)\n",
    "rdd = spark.read.parquet(\"hdfs://spark-master:9000/user/hadoop/input/TLC_Tripdata_Jan_2024.parquet\").rdd\n",
    "\n",
    "# 데이터 클리닝\n",
    "rdd_clean = rdd.filter(lambda x: x['base_passenger_fare'] is not None and x['trip_miles'] is not None) \\\n",
    "    .filter(lambda x: x['base_passenger_fare'] > 0 and x['trip_miles'] > 0)\n",
    "\n",
    "# 변환 로직\n",
    "rdd_transformed = rdd_clean.map(lambda x: Row(\n",
    "    date=x['pickup_datetime'].date(),  # datetime 객체에서 date() 메서드 호출\n",
    "    fare_amount=float(x['base_passenger_fare']),\n",
    "    trip_distance=float(x['trip_miles'])\n",
    "))\n",
    "\n",
    "# 집계 로직\n",
    "rdd_aggregated = rdd_transformed.map(lambda x: (x.date, (1, x.fare_amount, x.trip_distance))) \\\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2])) \\\n",
    "    .map(lambda x: (x[0], x[1][0], x[1][1], x[1][2] / x[1][0])) \\\n",
    "    .map(lambda x: Row(date=x[0], total_trips=x[1], total_revenue=x[2], avg_trip_distance=x[3]))\n",
    "\n",
    "# 전체 집계 결과\n",
    "total_trips = rdd_transformed.count()\n",
    "total_revenue = rdd_transformed.map(lambda x: x.fare_amount).reduce(lambda a, b: a + b)\n",
    "avg_trip_distance = rdd_transformed.map(lambda x: x.trip_distance).mean()\n",
    "\n",
    "# 일별 집계 결과 표시\n",
    "for row in rdd_aggregated.collect():\n",
    "    print(row)\n",
    "\n",
    "# 전체 집계 결과 표시\n",
    "print(f\"Total Trips: {total_trips}\")\n",
    "print(f\"Total Revenue: {total_revenue}\")\n",
    "print(f\"Average Trip Distance: {avg_trip_distance}\")\n",
    "\n",
    "# 결과 저장\n",
    "output_path = \"hdfs://spark-master:9000/output\"\n",
    "rdd_aggregated.map(lambda x: (x.date, x.total_trips, x.total_revenue, x.avg_trip_distance)).saveAsTextFile(f\"{output_path}/daily_metrics_rdd\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
